---
layout: post
title:  "Scrape Deep"
date:   2017-07-04 18:18:27 +0000
---


Want to have a good time?  Go scrape some websites.  Though this may sound like a surface activity, like, for example, you scrape your plate clean, leaving no food on it's surface (or, "no dessert for you"), it is anything but.  A more appropriate name might be "mining websites" (this was my first thought, however, I soon discovered that web mining is a 'thing', see definitions note below).  In my beginner's experience, to find what you want you don't just skim the surface (as the word "scraping" would imply) of website CSS, you go deeper and deeper till you find precisely what you need.  Another way of thinking of it might be web surgery.  You want targeted precision, as opposed to butchery, access to your exact need.  We like our surgeons to be precise in this way, am I right?

Now, not to wear you out with metaphors, but. . . here comes another one, hunting.  Being a good web scraper is like being a good hunter, you've got to know how to track it, know how to trap it (we only do catch, copy and release).  Of course, web-dev tools and the ability to inspect are essential here, I've also found pry to be VERY handy in scaping as well (to the point that I almost feel an unwillingness to scrape without pry).  Pry lets you know, quickly, if you trapped what you were actually aiming for and only what you're aiming for (no good to trap all the forrest creatures when all you want is one bear).  Thankfully,  tracking your CSS selector prey is about the equivalent of tracking a peg-legged pirate strolling down a beach with a 5 minute lead at low tide, its in the bag (at least, so far as my limited experience has taught me).  It's the actual scraping code that can get tricky.  It needs to be flexible and easily adaptable to the regularly updated and fluid websites you're scraping (else, as we say in these parts, "that dog just won't hunt", at least, not for long).  You have to stay on top of any changes and be able to translate those changes into your scraper code.

Ok, now that your metaphor induced eye-rolling and involuntary groaning has reached migrane levels, lets divert into a little discussion of scraping etiquette.  First, lets consider some ways in which scraping can veer into unethical territory.  Ultra-fast reading/scraping of websites can cause difficulties for their servers, leading to a degraded performance in the website.  We can alleviate this by building delays into our scrapers and by setting scrapers to work during low traffic hours.  Another, maybe more obvious veer into the unethical would be scraping a website's contents and then posting it as your own, this is copyright, intellectual property and trademark territory and laws still apply on the internet.  Make every effort to comply with the stated terms of service for a website and ensure that your activity does not affect other users of the website you are scraping.  Especially make sure you are familiar with and in compliance to the website's robots.txt, should they have one.  Clearly identify yourself in the UserAgent header, this way the site can see who you are and either restrict or allow certain areas of their site to you.  Another nice practice, use compression gzip/deflate if the site supports it, this will save time and saves the site's bandwidth.  All in all, when in doubt, COMMUNICATE clearly and respectfully with the website's administrators and abide by thier wishes, and . . . "scrape unto others as you would have them scrape unto you" (sorry, couldn't resist).

Now, since I am in the grammar phase of my code learning adventure, I offer some definitions to peruse (but mostly for my own edification).

**Data mining** is the computing process of discovering patterns in large data sets involving methods at the intersection of machine learning, statistics, and database systems. It is an interdisciplinary subfield of computer science. The overall goal of the data mining process is to extract information from a data set and transform it into an understandable structure for further use. Aside from the raw analysis step, it involves database and data management aspects, data pre-processing, model and inference considerations, interestingness metrics, complexity considerations, post-processing of discovered structures, visualization, and online updating. Data mining is the analysis step of the "knowledge discovery in databases" process, or KDD. ([https://en.wikipedia.org/wiki/Data_mining)

 **Robots.txt**, is a standard used by websites to communicate with web crawlers and other web robots. The standard specifies how to inform the web robot about which areas of the website should not be processed or scanned.
 
**Screen scraping** (most view screen scraping as one and the same with web scraping) is normally associated with the programmatic collection of visual data from a source, instead of parsing data as in Web scraping. Originally, screen scraping referred to the practice of reading text data from a computer display terminal's screen. This was generally done by reading the terminal's memory through its auxiliary port, or by connecting the terminal output port of one computer system to an input port on another. The term screen scraping is also commonly used to refer to the bidirectional exchange of data. This could be the simple cases where the controlling program navigates through the user interface, or more complex scenarios where the controlling program is entering data into an interface meant to be used by a human.(https://en.wikipedia.org/wiki/Data_scraping)

**Web crawler**, sometimes called a spider, is an Internet bot that systematically browses the World Wide Web they are used for web scraping but are mostly used  for the purpose of Web indexing (web spidering). Web search engines and some other sites use Web crawling or spidering software to update their web content or indices of others sites' web content. Web crawlers can copy all the pages they visit for later processing by a search engine which indexes the downloaded pages so the users can search much more efficiently. Crawlers consume resources on the systems they visit and often visit sites without approval. Issues of schedule, load, and "politeness" come into play when large collections of pages are accessed. Mechanisms exist for public sites not wishing to be crawled to make this known to the crawling agent. For instance, including a robots.txt file can request bots to index only parts of a website, or nothing at all. Crawlers can validate hyperlinks and HTML code. They can also be used for web scraping (https://en.wikipedia.org/wiki/Web_crawler)

**Web mining** - is the application of data mining techniques to discover patterns from the World Wide Web. As the name proposes, this is information gathered by mining the web. It makes utilization of automated apparatuses to reveal and extricate data from servers and web reports, and it permits organizations to get to both organized and unstructured information from browser activities, server logs, website and link structure, page content and different sources. Web mining can be divided into three different types â€“ Web usage mining, Web content mining and Web structure mining. (https://en.wikipedia.org/wiki/Web_mining)

**Web Scraping** (also termed Web Data Extraction, Web Harvesting etc.) is a technique employed to extract large amounts of data from websites whereby the data is extracted and saved to a local file in your computer or to a database in table (spreadsheet) format. (https://www.webharvy.com/articles/what-is-web-scraping.html)

Additional resources:
https://stackoverflow.com/questions/2022030/web-scraping-etiquette
https://quickleft.com/blog/is-web-scraping-ethical/


